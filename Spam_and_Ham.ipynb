{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Applied-Machine-Learning-2022/final-project-group6-morganstate/blob/main/Spam_%26_Ham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrqYtuaDgJX-"
      },
      "source": [
        "### Prepping the `DataFrame`\n",
        "\n",
        "When opening the colab it will often not include our spam dataset file.  \n",
        "Add Spam.csv to the folder first for now, while I upload it to kaggle and create the import commands in the meantime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNe6L2ddnQ7y"
      },
      "source": [
        "The Spam Dataset originally uses the wrong encoding for the excel sheet.\n",
        "Instructions to fix the data set are [here](https://medium.com/code-kings/python3-fix-unicodedecodeerror-utf-8-codec-can-t-decode-byte-in-position-be6c2e2235ee)\n",
        "  \n",
        "  The steps include:  \n",
        "\n",
        "  1. Right click span and click open with -> Notepad\n",
        "  1. Click file -> save as -> and keep the name the same but there will be an encoding option that is set to ANSI. Change it to UTF-8\n",
        "  1. Click Save. It will ask if you want to overwrite spam, click yes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyjvgmAcSN5a",
        "outputId": "d8619c3f-25fb-4cb6-fd55-d60bf7ca6c4c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# uploaded the dataframe using pandas read_csv\n",
        "df = pd.read_csv('spam.csv')\n",
        "\n",
        "# look at what columns we have\n",
        "df.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "cGocqUeYHonO",
        "outputId": "3a2dd935-0a6f-40a6-b0a4-cd501bd513a0"
      },
      "outputs": [],
      "source": [
        "# displays the first 5 entries of our dataframe\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqUdZlmTHonO",
        "outputId": "e2364271-9eec-41fe-ef63-b22d36b66667"
      },
      "outputs": [],
      "source": [
        "# displays a full summary of our dataframe\n",
        "# showing our column names and their index, non-null count and the datatype\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "1eOgCsn2OOnk",
        "outputId": "289ed03f-b1e0-4a24-889d-3b4e5dc7aa44"
      },
      "outputs": [],
      "source": [
        "# check the descriptions of dataframe\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PUaX0-fPLth",
        "outputId": "9a28b683-e342-4b41-d8a2-7bf5185dad07"
      },
      "outputs": [],
      "source": [
        "# missing values in these unnamed columns we dont need so lets drop them from our dataframe\n",
        "df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5JR2pcWnHonR",
        "outputId": "cae784c6-a7c1-4498-9b5d-589bae21f6c7"
      },
      "outputs": [],
      "source": [
        "# dropped these columns since we do not need them for detecting spam\n",
        "df = df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yNKOLHkwIoYS",
        "outputId": "b179b091-1d36-4da1-aeb5-9c20fadde8cc"
      },
      "outputs": [],
      "source": [
        "# changing the label column values to 0 and 1 for spam or ham\n",
        "column = 'label'\n",
        "for k, v in {'ham': 0, 'spam': 1}.items():\n",
        "  df.loc[df[column] == k, column] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7dozJCJybEN"
      },
      "outputs": [],
      "source": [
        "# lets split our data now into X and y\n",
        "X = df['message'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# if you remember when checked the .info() of our dataframe all of our datatypes were of type Object\n",
        "# above y is just a list of numbers stored as objects, so we cast it as type int\n",
        "y = y.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op_-EQt8yqwr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# here we then split our data again by 30% and 70%\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "vS1W53zE1JEb",
        "outputId": "9259c379-2087-490a-be5d-ce85c22b37f7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# next we are going to clean the data by removing punctations, URLs and numbers and we are going to change everyting to lowercase\n",
        "\n",
        "def clean_email(msg):\n",
        "  # turns the text into lowercase\n",
        "  msg = msg.lower()\n",
        "  # removes special characters\n",
        "  msg = re.sub(r'[^0-9a-zA-Z]', ' ', msg)\n",
        "  # this removes the whitespace\n",
        "  msg = ''.join(word + (' ' * random.randint(1, 10)) for word in msg.split(' '))\n",
        "  return msg\n",
        "\n",
        "df['message'] = df['message'].apply(clean_email)\n",
        "df.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCAVNZx93Bd1",
        "outputId": "41cfac88-0e91-4d2f-d7c5-3d9ad2507998"
      },
      "outputs": [],
      "source": [
        "# check the size of the data we are training and testing\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM890LOCzvtY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# in order to train our model we must convert the text into a matrix of token counts\n",
        "# we can do so using the CountVectorizer function that sklearn provides\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X_train, X_test = cv.fit_transform(X_train), cv.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMCcideEw9lE",
        "outputId": "36c11aee-f7f5-43fb-b6fa-07e019886ddb"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# building our SVM model with a rbf kernel\n",
        "svm = SVC(kernel='rbf', random_state=0)\n",
        "svm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# finding the accuracy and f1-score of our model\n",
        "estimator = SVC()\n",
        "f1 = make_scorer(f1_score, average='micro')\n",
        "cross = cross_val_score(estimator, X_train, y_train, scoring=f1)\n",
        "# cross.mean()\n",
        "\n",
        "print('Accuracy: ', svm.score(X_test,y_test))\n",
        "print('F1-Score: ', cross.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing the predictions\n",
        "print(svm.predict(X_train))\n",
        "\n",
        "# printing the actual values and it matches with the first 3 and last 3 as we can see\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate the model on the training dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "pred = svm.predict(X_train)\n",
        "print(classification_report(y_train, pred))\n",
        "print()\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_train, pred))\n",
        "print()\n",
        "print('Accruracy: \\n', accuracy_score(y_train, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing the predictions\n",
        "print(svm.predict(X_test))\n",
        "\n",
        "# printing the actual values and it matches with the first 3 and last 3 as we can see\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate the model on the training dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "pred = svm.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "print()\n",
        "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
        "print()\n",
        "print('Accruracy: \\n', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# overfitting is when you have a model that tries to adapt itself too much to the data that we have\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import io\n",
        "import pydotplus\n",
        "\n",
        "from IPython.display import Image \n",
        "\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "\n",
        "# what is a good reason to choose a certain max depth indicator amount?\n",
        "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "dt.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        ")\n",
        "\n",
        "dot_data = io.StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    dt,\n",
        "    out_file=dot_data,\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree.plot_tree(dt)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision path code from ski-learn. Need to add to it to produce the outputs displayed in Project 1\n",
        "# It says to Select at least three observations from the test dataset where the target label is ‘spam’ and show the decision path.\n",
        "\n",
        "threshold = dt.tree_.threshold\n",
        "node_indicator = dt.decision_path(X_test)\n",
        "leaf_id = dt.apply(X_test)\n",
        "\n",
        "sample_id = 0\n",
        "# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
        "node_index = node_indicator.indices[\n",
        "    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
        "]\n",
        "\n",
        "print(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\n",
        "for node_id in node_index:\n",
        "    # continue to the next node if it is a leaf node\n",
        "    if leaf_id[sample_id] == node_id:\n",
        "        continue\n",
        "\n",
        "    # check if value of the split y_train for sample 0 is below threshold\n",
        "    if X_test[sample_id, y_train[node_id]] <= threshold[node_id]:\n",
        "        threshold_sign = \"<=\"\n",
        "    else:\n",
        "        threshold_sign = \">\"\n",
        "\n",
        "    print(\n",
        "        \"decision node {node} : (X_test[{sample}, {y_train}] = {value}) \"\n",
        "        \"{inequality} {threshold})\".format(\n",
        "            node=node_id,\n",
        "            sample=sample_id,\n",
        "            y_train=y_train[node_id],\n",
        "            value=X_test[sample_id, y_train[node_id]],\n",
        "            inequality=threshold_sign,\n",
        "            threshold=threshold[node_id],\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1 = make_scorer(f1_score, average='micro')\n",
        "estimator = tree.DecisionTreeClassifier()\n",
        "cross = cross_val_score(estimator, X_train, y_train, scoring=f1)\n",
        "\n",
        "decision_tr = DecisionTreeClassifier()\n",
        "# decision_tr.fit(X_train, y_train)\n",
        "# pred = decision_tr.predict(X_test)\n",
        "# print(f'{accuracy_score(y_test, pred)}')\n",
        "# print(f'{cross.mean()}')\n",
        "decision_tr.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = decision_tr.predict(X_test)\n",
        "\n",
        "# f1 score\n",
        "# maybe it changes based on which one we put first?\n",
        "score = f1_score(y_pred, y_test)\n",
        "\n",
        "# print\n",
        "print(\"Decision Tree F1 score: {:2f}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fFovYzrHonU",
        "outputId": "1f280fdd-60d9-4c0f-a383-82ce8d9906e8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "f1 = make_scorer(f1_score, average='micro')\n",
        "estimator = LogisticRegression()\n",
        "cross = cross_val_score(estimator, X_train, y_train, scoring=f1)\n",
        "\n",
        "\n",
        "logistic = LogisticRegression(solver='liblinear', penalty='l1')\n",
        "logistic.fit(X_train, y_train)\n",
        "pred = logistic.predict(X_test)\n",
        "print(f'{accuracy_score(y_test, pred)}')\n",
        "print(f'{cross.mean()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gkb26aA6Yx6"
      },
      "source": [
        "[Assistance 1](https://towardsdatascience.com/spam-detection-with-logistic-regression-23e3709e522)  \n",
        "[Assistance 2](https://pythonprogramminglanguage.com/logistic-regression-spam-filter/)  \n",
        "[Assistance 3](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)  \n",
        "Attatched above are basically what we can referance for the logistical and other regressive types. Vectorization and data preprocessing seems important will address later."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Spam_&_Ham.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "781f1da9ce4a1003f4ec3a305fd0110bd3d215c42c633449f3b96759b3d51be1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
